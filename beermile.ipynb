{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Beermile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import requests\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_base = 'https://www.beermile.com/display/'\n",
    "max_event_id = 151381"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Find which events are actually races\n",
    "is_event = 0\n",
    "not_event = 0\n",
    "\n",
    "\n",
    "# Look for all events by numerical ID\n",
    "for i in range(1, max_event_id):\n",
    "    # Website uses sequential \n",
    "    event = f'event_{i}'\n",
    "    url = f'{url_base}{event}'\n",
    "    result = requests.get(url)\n",
    "    page = urlopen(result.url).read()\n",
    "    a = bs(page, 'html.parser')\n",
    "    \n",
    "    event_type = False\n",
    "    if (a.title.string == 'Race Not Available'):\n",
    "        not_event += 1\n",
    "        with open('not_events.txt', 'a') as g:\n",
    "            g.write(f'{i}\\n')\n",
    "    else:\n",
    "        is_event += 1\n",
    "        event_type = True\n",
    "        with open('events.txt', 'a') as f:\n",
    "            f.write(f'{i}\\n')\n",
    "            \n",
    "    print(f'\\r #{i:06d} Events: {is_event:<6} - Not Events: {not_event:<6}', end='') \n",
    "    sys.stdout.flush()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data Into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_soup_list(soup_list):\n",
    "    clean_soup_list = [s.text.strip() for s in soup_list]\n",
    "    return clean_soup_list\n",
    "\n",
    "def get_new_cols(headers, columns=[]):\n",
    "    '''\n",
    "    Use headers (from a table) Soup to find columns not already defined  \n",
    "    '''\n",
    "    new_columns = []\n",
    "    headers = clean_soup_list(headers)\n",
    "    for col in headers:\n",
    "        if col not in columns:\n",
    "            new_columns.append(col)\n",
    "    return new_columns\n",
    "\n",
    "def get_data_rows(event_rows):\n",
    "    data = []\n",
    "    is_header = True\n",
    "    for row in event_rows:\n",
    "        # Skip over first row (since it's the header)\n",
    "        if (is_header):\n",
    "            is_header = False\n",
    "        else:\n",
    "            cols = row.find_all('td')\n",
    "            # TODO: Determine how to \"clean\" string\n",
    "            cols = [ele.text.strip() for ele in cols]\n",
    "            row = [ele for ele in cols]\n",
    "            data.append(row)\n",
    "    return data\n",
    "\n",
    "def add_soup_to_df(events_soup, df):\n",
    "    '''\n",
    "    Add data (and potentially new columns) from Soup to df\n",
    "    '''\n",
    "    # Add new columns to df\n",
    "    columns = df.columns\n",
    "    headers = events_soup.find_all('tr')[0].find_all('td')\n",
    "    new_columns = get_new_cols(headers, columns)\n",
    "    for c in new_columns:\n",
    "        df[c] = np.nan\n",
    "    # Add data to df\n",
    "    data_rows = events_soup.find_all('tr')\n",
    "    new = get_data_rows(data_rows)\n",
    "    df = df.append(\n",
    "        pd.DataFrame(new, columns=clean_soup_list(headers)),\n",
    "        sort=False\n",
    "    )\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate through events list to get data into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "url_event_base = 'https://www.beermile.com/display/event_'\n",
    "cols = []\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for n_event in open('events.txt'):\n",
    "    url = f'{url_event_base}{n_event}'\n",
    "    page = urlopen(url).read()\n",
    "    soup = bs(page, 'html.parser')\n",
    "    \n",
    "    df = add_soup_to_df(soup,df)\n",
    "    # TEST: Only go through 5 results\n",
    "    count += 1\n",
    "    if count >5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display DataFrame created\n",
    "display(df.head())\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process data parallel (experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_df(event_ids):\n",
    "    count = 0\n",
    "    url_event_base = 'https://www.beermile.com/display/event_'\n",
    "    cols = []\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    for n_event in event_ids:\n",
    "        url = f'{url_event_base}{n_event}'\n",
    "        page = urlopen(url).read()\n",
    "        soup = bs(page, 'html.parser')\n",
    "\n",
    "        df = add_soup_to_df(soup,df)\n",
    "        # TEST: Only go through 5 results\n",
    "        count += 1\n",
    "        if count >5:\n",
    "            break\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_events = [n for n in open('events.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = Pool()\n",
    "# TODO: Read from file\n",
    "\n",
    "process_n = 5\n",
    "A = all_events[:process_n]\n",
    "B = all_events[process_n:process_n*2]\n",
    "C = all_events[process_n*2:process_n*3]\n",
    "D = all_events[process_n*3:process_n*4]\n",
    "\n",
    "\n",
    "result1 = pool.apply_async(parallel_df, [A])\n",
    "result2 = pool.apply_async(parallel_df, [B])\n",
    "result3 = pool.apply_async(parallel_df, [C])\n",
    "result4 = pool.apply_async(parallel_df, [D])\n",
    "\n",
    "df = result1.get(timeout=20)\n",
    "df = df.append(result2.get(timeout=20),sort=False)\n",
    "df = df.append(result3.get(timeout=20),sort=False)\n",
    "df = df.append(result4.get(timeout=20),sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
